# -*- coding: utf-8 -*-
"""Preprocessing code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pbyQ0Wu-qJdtPHcPDN3LVKDH6R51lDXN
"""

import pandas as pd
import numpy as np

# Load the dataset
# If you copied the path, paste it inside the quotes below
file_path = 'Dataset_ATS_v2 (2).csv'
df = pd.read_csv(file_path)

# Display the first 5 rows to ensure it loaded correctly
print("Dataset Preview:")
display(df.head())

# Check for missing values and data types
print("\nDataset Info:")
df.info()

"""Explanation:

import pandas as pd: This loads the library we use for data manipulation (DataFrames).

pd.read_csv(): This reads your file into a table format that Python can understand.

df.info(): This is crucial for a Data Engineer. It tells us if there are any null values and confirms the data types (e.g., whether a number is being read as a string).
"""

from sklearn.preprocessing import LabelEncoder

# 1. Initialize LabelEncoder for binary categories
le = LabelEncoder()

# List of columns with exactly two unique values
binary_cols = ['gender', 'Dependents', 'PhoneService', 'MultipleLines', 'Churn']

# Apply Label Encoding (turns 'Yes/No' into '1/0', etc.)
for col in binary_cols:
    df[col] = le.fit_transform(df[col])

# 2. One-Hot Encoding for multi-class columns
# This creates separate columns for 'InternetService' and 'Contract' options.
# 'drop_first=True' is used to avoid redundancy (The Dummy Variable Trap).
df = pd.get_dummies(df, columns=['InternetService', 'Contract'], drop_first=True)

# Display the first 5 rows to see the transformation
print("Encoded Data Preview:")
display(df.head())

# Show the new column names
print("\nNew Columns after Encoding:")
print(df.columns.tolist())

"""Explanation:

LabelEncoder: This is used for binary columns (where there are only two choices). For example, it transforms "Female" to 0 and "Male" to 1.

pd.get_dummies: This is used for "Nominal" data where there is no specific order (like types of Internet Service). It creates a new column for each category.

drop_first=True: This is a data engineering best practice. If a customer doesn't have a "One year" or "Two year" contract, we mathematically know they have a "Month-to-month" contract. Dropping the first column prevents redundant data, which helps the models perform better later.
"""

from sklearn.model_selection import train_test_split

# 1. Define Features (X) and the Target Variable (y)
# X contains all columns except 'Churn'
X = df.drop('Churn', axis=1)
# y contains only the 'Churn' column (what we want to predict)
y = df['Churn']

# 2. Split the data
# test_size=0.2 means 20% of the data goes to testing and 80% to training.
# random_state=42 ensures you get the same split every time you run this.
# stratify=y is CRITICAL: it ensures the percentage of churners is the same in both sets.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# 3. Print the results to verify
print("Data Splitting Complete!")
print(f"Total samples: {len(df)}")
print(f"Training samples: {len(X_train)}")
print(f"Testing samples: {len(X_test)}")

"""Explanation:

X and y: In data science, X represents the inputs (like tenure and contract type), and y represents the outcome (Churn).

test_size=0.2: We hold back 20% of the data. This unseen data will be the "final exam" for the predictive models later.

stratify=y: Customer churn datasets are usually imbalanced (more people stay than leave). If we don't stratify, we might accidentally get a test set with no churners at all, making it impossible to evaluate the model fairly.
"""

from sklearn.preprocessing import StandardScaler

# 1. Identify numerical columns to scale
num_cols = ['tenure', 'MonthlyCharges']

# 2. Initialize the Scaler
scaler = StandardScaler()

# 3. Fit on Training data ONLY and transform both
# We use .copy() to avoid SettingWithCopy warnings in Pandas
X_train_scaled = X_train.copy()
X_test_scaled = X_test.copy()

X_train_scaled[num_cols] = scaler.fit_transform(X_train[num_cols])
X_test_scaled[num_cols] = scaler.transform(X_test[num_cols])

# 4. Prepare final dataframes (adding target back for the Analysts)
train_final = X_train_scaled.copy()
train_final['Churn'] = y_train

test_final = X_test_scaled.copy()
test_final['Churn'] = y_test

# 5. Save to CSV files for the rest of the team
train_final.to_csv('preprocessed_train_data.csv', index=False)
test_final.to_csv('preprocessed_test_data.csv', index=False)

print("Feature Scaling Complete!")
print("Final CSV files generated: 'preprocessed_train_data.csv' and 'preprocessed_test_data.csv'")
display(train_final.head())

"""Explanation:

StandardScaler: This converts your data so that the average (mean) is 0 and the spread (standard deviation) is 1. This is essential for the Clustering Analyst (Step 3 of the project) because algorithms like K-Means rely on distances between points.

Fit on Train, Transform Test: This is a golden rule in Data Engineering. We calculate the "average" using only the training data to ensure the model doesn't know anything about the test data's distribution beforehand.
"""

from sklearn.preprocessing import StandardScaler

# 1. We start with the 'df' from Step 2 (which already has encoding applied)
df_full_preprocessed = df.copy()

# 2. Identify numerical columns to scale
num_cols = ['tenure', 'MonthlyCharges']

# 3. Initialize a new Scaler for the full dataset
# This ensures the scaling statistics (mean and standard deviation)
# are calculated based on the entire population of 7,043 rows.
full_scaler = StandardScaler()

# 4. Apply scaling to the full dataset
df_full_preprocessed[num_cols] = full_scaler.fit_transform(df[num_cols])

# 5. Save the complete preprocessed data to a single CSV file
df_full_preprocessed.to_csv('preprocessed_full_dataset.csv', index=False)

# 6. Verify the results
print("Full Preprocessed Dataset Created!")
print(f"Total rows: {df_full_preprocessed.shape[0]}")
print(f"Total columns: {df_full_preprocessed.shape[1]}")
display(df_full_preprocessed.head())

"""While having separate training and testing sets is essential for building and validating machine learning models, having a single, unified preprocessed file is often useful for the Business Analyst to perform global reporting or for the Clustering Analyst to look at the entire customer base at once.

In this step, we will apply the scaling to the entire dataset (all 7,043 rows) and export it as one file.
"""